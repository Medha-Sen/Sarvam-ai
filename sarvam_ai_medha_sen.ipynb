{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3381be-a422-4402-a072-717a7693e847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T05:31:06.365721Z",
     "iopub.status.busy": "2025-04-02T05:31:06.365502Z",
     "iopub.status.idle": "2025-04-02T05:31:09.609706Z",
     "shell.execute_reply": "2025-04-02T05:31:09.608967Z",
     "shell.execute_reply.started": "2025-04-02T05:31:06.365704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\medha\\anaconda3\\lib\\site-packages (1.24.4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tqdm in c:\\users\\medha\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\medha\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\medha\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\medha\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from gensim) (2.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\medha\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\medha\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: simpful==2.12.0 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso==1.8.1 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\medha\\anaconda3\\lib\\site-packages (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\medha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tqdm gensim scikit-learn #importing essential libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\medha\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wget #ignore on a Linux system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c58856-4d0c-4f50-9730-8bf90b2298c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:45:07.451206Z",
     "iopub.status.busy": "2025-03-31T10:45:07.450892Z",
     "iopub.status.idle": "2025-03-31T10:45:15.223198Z",
     "shell.execute_reply": "2025-03-31T10:45:15.222534Z",
     "shell.execute_reply.started": "2025-03-31T10:45:07.451178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under cc.en.300.vec.gz\n",
      "\n",
      "Saved under cc.hi.300.vec.gz\n"
     ]
    }
   ],
   "source": [
    "!python -m wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz # downloads pre-trained FastText Embeddings for English and Hindi respectively\n",
    "!python -m wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49381e-5d7d-4d5d-b55b-0eac7ab8890e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T10:46:04.827644Z",
     "iopub.status.busy": "2025-03-31T10:46:04.827064Z",
     "iopub.status.idle": "2025-03-31T10:46:05.072479Z",
     "shell.execute_reply": "2025-03-31T10:46:05.071862Z",
     "shell.execute_reply.started": "2025-03-31T10:46:04.827616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under en-hi.txt\n"
     ]
    }
   ],
   "source": [
    "!python -m wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt #downloads the MUSE bilingual dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d359fb",
   "metadata": {},
   "source": [
    "### Reads and processes the dictionary into a list of word pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814c7cf4-55fa-4ac9-a538-36e86e06c83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T05:31:22.748808Z",
     "iopub.status.busy": "2025-04-02T05:31:22.748218Z",
     "iopub.status.idle": "2025-04-02T05:31:22.784115Z",
     "shell.execute_reply": "2025-04-02T05:31:22.783705Z",
     "shell.execute_reply.started": "2025-04-02T05:31:22.748784Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_hind_pair = {}\n",
    "with open('en-hi.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        eng, hin = line.strip().split()\n",
    "        eng_hind_pair[eng] = hin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7d0fff-f9e9-4c8b-9882-46cf0a657345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T05:31:26.404981Z",
     "iopub.status.busy": "2025-04-02T05:31:26.404461Z",
     "iopub.status.idle": "2025-04-02T05:31:26.407856Z",
     "shell.execute_reply": "2025-04-02T05:31:26.407532Z",
     "shell.execute_reply.started": "2025-04-02T05:31:26.404953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of English-Hindi word pairs are 31719\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of English-Hindi word pairs are\" , len(eng_hind_pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf674d",
   "metadata": {},
   "source": [
    "### Loads embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92187cd9-b391-4f4e-a47b-39b439b6cef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T05:31:36.801872Z",
     "iopub.status.busy": "2025-04-02T05:31:36.801069Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "eng_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz')\n",
    "hin_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b243f",
   "metadata": {},
   "source": [
    "### Extracts aligned word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82069e93-be2a-4bf5-b731-d30e9c454bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Extract the embeddings for the bilingual lexicon pairs\n",
    "# Extract aligned word vectors\n",
    "common_words = [word for word in eng_hind_pair if word in eng_embeddings and eng_hind_pair[word] in hin_embeddings]\n",
    "\n",
    "# Extract aligned word vectors with same shape\n",
    "X = np.array([eng_embeddings[word] for word in common_words])  # Shape: (N, 300)\n",
    "Y = np.array([hin_embeddings[eng_hind_pair[word]] for word in common_words])  # Shape: (N, 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ad09d",
   "metadata": {},
   "source": [
    "### Procrustes alignment algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd6e6507-1f9e-4463-9851-815adcd01de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_algo(X, Y):\n",
    "    A, _, Bt = np.linalg.svd(np.dot(X.T, Y))\n",
    "    U = np.dot(A, Bt)\n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a78e2",
   "metadata": {},
   "source": [
    "### Computes Procrustes alignment and transforms english embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3bb7ae3-0055-4a67-8818-b3b3ae2fc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the optimal orthogonal matrix using the bilingual lexicon\n",
    "U = procrustes_algo(X, Y)\n",
    "\n",
    "# Align the English embeddings with the learned transformation matrix\n",
    "aligned_eng_emb = np.dot(eng_embeddings.vectors, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfcfb94",
   "metadata": {},
   "source": [
    "### Word translation using nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "353a456d-25c5-4db0-be18-97bd6810418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Fit nearest neighbor search on the aligned Hindi embeddings\n",
    "hin_vectors = np.array([hin_embeddings[word] for word in hin_embeddings.key_to_index])  # Hindi embedding matrix\n",
    "hin_words = list(hin_embeddings.key_to_index.keys())  # Hindi word list\n",
    "\n",
    "# Use Nearest Neighbors to find closest Hindi words\n",
    "def translate_word(word, top_k=5):\n",
    "    if word not in eng_embeddings:\n",
    "        return []\n",
    "    \n",
    "    # Align the English word vector\n",
    "    eng_vector = np.dot(eng_embeddings[word], U).reshape(1, -1)\n",
    "\n",
    "    # Find nearest Hindi words\n",
    "    nn_model = NearestNeighbors(n_neighbors=top_k, metric='cosine').fit(hin_vectors)\n",
    "    distances, indices = nn_model.kneighbors(eng_vector)\n",
    "\n",
    "    return [hin_words[idx] for idx in indices[0]]  # Return top-k translated words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff34ee8-b94d-4351-a03a-92660aaf41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hindi translations for 'life': ['जीवन', 'जिंदगी', 'जिनगीभर', 'जिन्दगी', 'संघर्षमयी']\n"
     ]
    }
   ],
   "source": [
    "# Test an example English word\n",
    "example_word = \"life\"\n",
    "top_k_predictions = translate_word(example_word, top_k=5)\n",
    "\n",
    "print(f\"Top 5 Hindi translations for '{example_word}': {top_k_predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6587f13",
   "metadata": {},
   "source": [
    "### Evaluating translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928d8f46-47be-43f6-9df3-3f922c2114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_translation(test_dict, aligned_eng_emb, eng_embeddings, hin_embeddings, k=5):\n",
    "    correct_at_1 = 0\n",
    "    correct_at_5 = 0\n",
    "    total = len(test_dict)\n",
    "    \n",
    "    for eng_word, hin_word in test_dict.items():  # Use .items() since test_dict is a dictionary\n",
    "        if eng_word not in eng_embeddings.key_to_index or hin_word not in hin_embeddings.key_to_index:\n",
    "            continue  # Skip words not in vocabulary\n",
    "        \n",
    "        # Get aligned English vector\n",
    "        eng_vector = aligned_eng_emb[eng_embeddings.key_to_index[eng_word]]\n",
    "\n",
    "        # Compute cosine similarity with all Hindi embeddings\n",
    "        similarities = cosine_similarity(eng_vector.reshape(1, -1), hin_embeddings.vectors)\n",
    "\n",
    "        # Get top-k closest words\n",
    "        top_k_indices = similarities[0].argsort()[-k:][::-1]\n",
    "        top_k_words = [hin_embeddings.index_to_key[i] for i in top_k_indices]\n",
    "\n",
    "        # Compute accuracy\n",
    "        if hin_word == top_k_words[0]:\n",
    "            correct_at_1 += 1\n",
    "        if hin_word in top_k_words:\n",
    "            correct_at_5 += 1\n",
    "\n",
    "    precision_at_1 = correct_at_1 / total\n",
    "    precision_at_5 = correct_at_5 / total\n",
    "    return precision_at_1, precision_at_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950f210-4449-4228-9f25-bb03781a178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of 2000 word pairs from MUSE dictionary\n",
    "muse_test_dict = dict(list(eng_hind_pair.items())[:2000])  # Convert to dict for easy lookup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7bdfb",
   "metadata": {},
   "source": [
    "### Precision@1 and Precision@5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c48260-c36b-469d-ad7d-cf0464b5515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.3510\n",
      "Precision@5: 0.5650\n"
     ]
    }
   ],
   "source": [
    "precision_at_1, precision_at_5 = evaluate_translation(muse_test_dict, aligned_eng_emb, eng_embeddings, hin_embeddings)\n",
    "\n",
    "print(f'Precision@1: {precision_at_1:.4f}')\n",
    "print(f'Precision@5: {precision_at_5:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8debc4",
   "metadata": {},
   "source": [
    "### Cosine Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "871c0d73-683e-4d1b-89dd-65bf4b9bf2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(test_dict, aligned_eng_emb, eng_embeddings, hin_embeddings):\n",
    "    cosine_similarities = {}\n",
    "\n",
    "    for eng_word, hin_word in test_dict.items():\n",
    "        if eng_word not in eng_embeddings.key_to_index or hin_word not in hin_embeddings.key_to_index:\n",
    "            continue  # Skip words not in vocab\n",
    "        \n",
    "        # Get vectors\n",
    "        eng_vector = aligned_eng_emb[eng_embeddings.key_to_index[eng_word]]\n",
    "        hin_vector = hin_embeddings[hin_embeddings.key_to_index[hin_word]]\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = np.dot(eng_vector, hin_vector) / (np.linalg.norm(eng_vector) * np.linalg.norm(hin_vector))\n",
    "\n",
    "        cosine_similarities[eng_word] = similarity\n",
    "\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf559562-e389-4e7c-81e4-a2fc00ac11ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for 'and': 0.4399\n",
      "Cosine similarity for 'was': 0.4678\n",
      "Cosine similarity for 'for': 0.4909\n",
      "Cosine similarity for 'that': 0.4552\n",
      "Cosine similarity for 'with': 0.4831\n",
      "Cosine similarity for 'from': 0.3387\n",
      "Cosine similarity for 'this': 0.4511\n",
      "Cosine similarity for 'utc': 0.3704\n",
      "Cosine similarity for 'his': 0.4868\n",
      "Cosine similarity for 'not': 0.6378\n"
     ]
    }
   ],
   "source": [
    "cosine_sims = compute_cosine_similarity(muse_test_dict, aligned_eng_emb, eng_embeddings, hin_embeddings)\n",
    "\n",
    "# Print a few example similarities\n",
    "for word, sim in list(cosine_sims.items())[:10]:  # Show top 10 examples\n",
    "    print(f\"Cosine similarity for '{word}': {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de7805",
   "metadata": {},
   "source": [
    "### Ablation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "295cb4f4-1d4e-4300-812c-8020c615f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_study(eng_hind_pair, eng_embeddings, hin_embeddings, size):\n",
    "    results = {}\n",
    "\n",
    "    subset_dict = dict(list(eng_hind_pair.items())[:size])\n",
    "\n",
    "    # Extract aligned word vectors\n",
    "    common_words = [word for word in subset_dict if word in eng_embeddings and subset_dict[word] in hin_embeddings]\n",
    "    X_subset = np.array([eng_embeddings[word] for word in common_words])\n",
    "    Y_subset = np.array([hin_embeddings[subset_dict[word]] for word in common_words])\n",
    "\n",
    "    # Compute Procrustes transformation\n",
    "    U_subset = procrustes_algo(X_subset, Y_subset)\n",
    "    aligned_eng_emb_subset = np.dot(eng_embeddings.vectors, U_subset)\n",
    "\n",
    "    # Evaluate performance\n",
    "    precision_at_1, precision_at_5 = evaluate_translation(subset_dict, aligned_eng_emb_subset, eng_embeddings, hin_embeddings)\n",
    "\n",
    "    # Store results\n",
    "    results[size] = (precision_at_1, precision_at_5)\n",
    "    print(f\"Dictionary Size: {size} → Precision@1: {precision_at_1:.4f}, Precision@5: {precision_at_5:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b00cbdb-7e58-422a-87bf-b7c0b38606f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size: 5000 → Precision@1: 0.4296, Precision@5: 0.6404\n"
     ]
    }
   ],
   "source": [
    "ablation_results_5000 = run_ablation_study(eng_hind_pair, eng_embeddings, hin_embeddings, size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06e58467-fb89-4b45-8689-aed3c5670c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size: 10000 → Precision@1: 0.3093, Precision@5: 0.4870\n"
     ]
    }
   ],
   "source": [
    "ablation_results_10000 = run_ablation_study(eng_hind_pair, eng_embeddings, hin_embeddings, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2cfdd81-b369-4c54-a76b-7dabe94ba6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size: 20000 → Precision@1: 0.1950, Precision@5: 0.3257\n"
     ]
    }
   ],
   "source": [
    "ablation_results_20000 = run_ablation_study(eng_hind_pair, eng_embeddings, hin_embeddings, size=20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
